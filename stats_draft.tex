\documentclass[titlepage, 12pt, twoside]{book}
\usepackage[hmarginratio=1:1]{geometry}
\usepackage{todonotes}
\usepackage{xcolor, sectsty}
\usepackage[hidelinks]{hyperref}
\usepackage{afterpage}
\usepackage[most]{tcolorbox}
\usepackage{titlesec}
\usepackage{tocloft}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[english]{babel}
\setlength{\columnsep}{1cm}

% ---------Organizing the Section Indentations ----------
\makeatletter
\renewcommand\section{\@startsection {section}{1}{\z@}%
	{-3.5ex \@plus -1ex \@minus -.2ex}%
	{2.3ex \@plus.2ex}%
	{\normalfont\Large\bfseries\raggedright}}
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
	{-3.25ex\@plus -1ex \@minus -.2ex}%
	{1.5ex \@plus .2ex}%
	{\normalfont\large\bfseries\raggedright}}
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
	{-3.25ex\@plus -1ex \@minus -.2ex}%
	{1.5ex \@plus .2ex}%
	{\normalfont\normalsize\bfseries\raggedright}}
\makeatother

% ---- Putting Page Numbers in Place -----------------------
\newcounter{nopage}
\newenvironment{nopage}
{\clearpage\stepcounter{nopage}%
	\renewcommand{\thepage}{NP\arabic{nopage}}%
	\thispagestyle{empty}}
{\clearpage\addtocounter{page}{-1}}

%----- Defining a color Blue that I like ----------
\definecolor{astral}{RGB}{46,116,181}
\subsectionfont{\color{astral}}
\sectionfont{\color{astral}}

% ---- Setting Cover Photo -----------------
\usepackage{eso-pic}
\newcommand\BackgroundPic{%
	\put(0,0){%
		\parbox[b][\paperheight]{\paperwidth}{%
			\vfill
			\centering
			\includegraphics[width=\paperwidth,height=\paperheight,%
			keepaspectratio]{background.png}%
			\vfill
}}}

% ---- Title Formtion ---------------
\date{}
\centering 
\title{%
	\vspace{10mm}
	\color{black} \textbf{Statistical Learning Modeling and Theory} \\
	\large \textbf{Coding Applications in R and Python \\
	with Illustrating Labs}}

%\author{    %
%	LH$^{1}$, LM$^{2}$, AA$^{3}$\\    % Optional Author Insert
%}



%------- Starting the textbook------------------
\begin{document}
	\AddToShipoutPicture*{\BackgroundPic} % Cover photo
	\pagenumbering{roman}  % Page number type before text starts (i, ii, iii, iv) 
	\maketitle
	
	\begin{nopage}
		\vspace*{\fill}
		\centering
	\color{astral}	\Large \textit{``Big data isn't about bits, \\     % Page with cheesy quote
			it's about talent.''} \\
		\vspace{1mm}
		-- Douglas Merill
		\vspace*{\fill}
	\end{nopage}	

	\addcontentsline{toc}{chapter}{Foreword}   % Optional Foreward Statement
	
	\addcontentsline{toc}{chapter}{Acknowledgement}  % Optional Acknowledgement statement
	%some agreement
	
\color{astral}\tableofcontents  % Setting table of contents
\mainmatter
	
% ----------INTRODUCTION-------------------------------------
	\chapter{Introduction}
	
	\vspace{50mm}
	
	\color{black} \paragraph{}   \textbf{General Outline for the Intro:} \\ 1. The growth of information. \\  2. The rise of data and AI;  "Data is the new oil" - Clive Humby, and "AI is the new electricity" - Andrew Ng  \\ 3. How statistical learning fits into the picture; what it does; and why it's important. \\ 4. What the book covers, and what the book can offer. \\ 5. Who the book is "for". \\6. How to use the book.


% ------------CHAPTER 1 ----------------------------------

	\color{astral}\chapter{Statistical Learning}

	 \section[The Basics of Statistical Learning ]{The Basics of Statistical Learning} % Example of a section 

	
\color{black} 	\paragraph{} Generally speaking, statistical learning is a construct that attempts to predict an outcome, or determine a relationship. The act of predicting an outcome falls under a type of learning called \textit{supervised learning}. Supervised learning requires that you have an \textit{output}, $Y$. Determining a relationship falls under a type of learning called \textit{unsupervised learning}, which does not require an output. Considering \textit{supervised learning}, the output variable $Y$, can be either \textit{quantitative}, or \textit{qualitative}. \textit{Quantitative} outputs are numeric and continuous, or close in nature. \textit{Qualitative} outputs are categorical, or discrete in nature. 

We introduce the notation for input variable(s), $X$: 

\vspace{1mm}
\begin{equation}	
	X = (X_1, X_2, ... , X_P).    
 \end{equation}
\vspace{-5mm}

It is worth taking the time to stop, and make a mental note of the various names that are synonomous to the term: \textit{variable}. Within the scope of statistical learning, the term \textit{variable} is interchangeable with \textit{predictors} (hence the P in $X_P$), \textit{independent variables}, or \textit{features}. While, the output variable is often referred to as the \textit{response}, or \textit{dependent variable}. 

\color{astral}\subsection[Types of Estimation]{Types of Estimation}
		
\color{black} \paragraph{} Supervised learning can be split into two different kinds of estimation: \textit{prediction} and \textit{inference}. When the time comes, you may determine that one estimation better frames your question than the other. Conversely, you may determine that your question is best answered with the utilization of both types of estimation. Nevertheless, both forms of estimation offer unique insight, and require unique paramaters. We discuss each in depth. 

\color{astral} \subsubsection*{Prediction}

\color{black} \paragraph{} The first type of estimation is referred to prediction, which is implemented when we have information about $X$, and we want to predict $Y$. Let's say that we have a quantative (numerical) response variable, $Y$, with predictors $X = (X_1, X_2, ... X_P)$. We can set up a very general equation that predicts $Y$ using

\begin{equation}
	\hat{Y} = \hat{f}(X) + \epsilon,
\end{equation} 

based on the assumption that there is in fact some relationship between $Y$, and 
$X = (X_1, X_2, ... X_P)$, which is assumed to have the form:
\begin{equation}
	Y = f (X) + \epsilon.
\end{equation} 
 
 Here, $\hat{Y}$ is our resulting prediction for $Y$, $\hat{f}$ is our estimate for $f$, which represents the systematic information that $X$ carries about $Y$, and $\epsilon$ is the random error term. The goal in a prediction envorinment is to have values for $\hat{Y}$ that are accurate with respect to $Y$, or the actual outcome. In this context, we are not concerned with the question of \textit{``what is the true form of f?"}, but rather, how accurate are our precictions of $Y$. Lastly, $\epsilon$ refers to the unavoidable error term that will surely be present in any prediction. This random error is independent of $X$, has a mean of zero, and can account for the randomness in the real-world that is naturally occurring. Since $\epsilon$ has a mean of xero, $Y$ can be predicted by using 
 
 \begin{equation}	
 \hat{Y} = \hat{f}(X)
 \end{equation} 
 
\color{astral}\subsubsection*{Inference}
		
\color{black} \paragraph{} Discussion about inference
% ------------CHAPTER 2 ----------------------------------		
%	\color{astral}\chapter{Linear Regression }
	
	
	
	
	
	
	
	
	
\end{document}