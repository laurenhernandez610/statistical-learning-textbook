{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924d220e",
   "metadata": {},
   "source": [
    "# Statistical Learning Overview\n",
    "\n",
    "\n",
    "<!---ISLR--->\n",
    "\n",
    "Statistical learning is a huge toolbox that allows us to understand data. There are two big ways to categorize these tools, supervised and unsupervised learning. **Supervised** learning consists of a model where you are trying to predict something based on the information you give it, also known as your inputs. **Unsupervised** learning is when you have those inputs but you don't really know what you are gonna get out of it. It gives us a way to build relationships within the data that we didn't know prior. \n",
    "\n",
    "\n",
    "## Terminology\n",
    "Some things we should get out of the way instantly is the way we describe our inputs and outputs. Inputs are almost always denoted using the symbols $X$. If you have more than out input they will be denoted accordingly with numbered subscripts. Naturally, the actual word input is not the only word used to describe the inputs. Other common terms for our $X$ are **predictors, independent variables, features, and variables**. The output is usually denoted by $Y$. It is what the model spits out after we shoved all of our $X$'s into it. Thus, the other ways you will see output described is **responses and dependent variable**. The relationship between $Y$ and $X$ can be written in the following way $$Y = f(X) + \\epsilon$$ where $f$ is a fixed unknown function of $X_{1}, \\dots, X_{p}$ and $\\epsilon$ is just an **error term** which we will go more into detail later, for now just know that it has a mean of 0. The goal is to use statistical learning tools to be able to estimate $f$. \n",
    "\n",
    "\n",
    "### Predicting\n",
    "When you have a set of $X$ variables available and you would like to predict $Y$.\n",
    "\n",
    "#### Error term\n",
    "Like I said earlier we are going to go more indepth into what exactly that error term is. This $\\epsilon$ is known as our **irreducible error**. Why is it irreducible? Well simply because no matter how amazing the model we make is, there will always be some sort of error, $\\epsilon$. I know that was very ambiguous but it will all make sense. We also have a **reducible error**, which is what our model aims to make as little as possible. We can always improve the accuracy of our model, thus making it reducible. You should know that $\\epsilon$ provides an upper bound for the accuracy of the predicted $Y$. \n",
    "\n",
    "### Infering\n",
    "When you want to understand the association between $X$ and $Y$. When you infer you want to know the exact model you are using to get these inferences. \n",
    "\n",
    "## Estimating our function\n",
    "As mentioned a lot of the time we want to estimate $f$. A lot of the methods used often have similar characteristics. There is always an assumed data set with $n$ data points. Normally these observations will be the **training data** in which we use to train or teach the model/method used to estimate $f$. Thus, in SL you want to find a predicted function $f$ such that $Y$ is approximately $f(X)$ for any observation in your set $(X,Y)$. \n",
    "\n",
    "### Parametric Approach\n",
    "1. Make an assumption about the function form or shap of $f$. \n",
    "2. Once you gave selected a model, a procedure that uses training data to fit or train the model is needed. \n",
    "\n",
    "The point of this approach is to reduce the problem of estimating $f$ down to estimating a set of parameters. A disadvantage of a parameteric approach is that the model used will not match the real form of $f$ which could lead to problems. Using a complex model can lead to **overfitting** which means they are going more towards the error term rather than it's real path. \n",
    "\n",
    "### Non-parametric Approach\n",
    "The difference between parameteric and non-parametric approaches is that in this approach an assumption is not made about the form of $f$ as it was made in the first step of the parametric form. Why is this advantagous? Because we are looking for the form of $f$ that doesn't follow a noisy path. The danger of the model not being close to the actual model is gone. Then why would we ever use parametetric approaches you ask? **MANY** data points are needed to find an accurate estimation of $f$. \n",
    "\n",
    "There are many trade offs to picking a flexible or inflexible model. The one you choose ultimately comes down to the problem you are dealing with. \n",
    "<!------->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0becacf3",
   "metadata": {},
   "source": [
    "# Python Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a34ddf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Important Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as mpl\n",
    "import pandas as pd\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfeb60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Commands and things you should know\n",
    "x = 30 ## assigning variables\n",
    "x = [1, 2, 3] ## creating a list\n",
    "length = len(x) ## length of our list\n",
    "np_x = np.array(x) ## Creating a numpy array from an existing list\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                 [1, 2, 3]]) ## Creating a Matrix\n",
    "sqrt = np.sqrt(matrix) ## Square root\n",
    "squared = matrix ** 2 ## Squaring \n",
    "mean = np.mean(matrix) ## Mean\n",
    "variance = np.var(matrix) ## Variance\n",
    "std = np.std(matrix) ## Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21661df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
